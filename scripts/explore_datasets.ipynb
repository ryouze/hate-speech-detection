{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: explore_datasets.ipynb\n",
    "\n",
    "This notebook is a simple exploration of the dataset. It is a good starting point to understand the data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from lib import filepaths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First version of the dataset (BAN-PL_1.zip)\n",
    "# - Upload Date: 16.08.2023\n",
    "# - Rows: 24,000\n",
    "# - Classes: 0 – non-harmful, 1 – harmful\n",
    "df1: pandas.DataFrame = pandas.read_csv(filepaths.datasets / \"BAN-PL_1.csv\")\n",
    "\n",
    "# Second version of the dataset (BAN-PL_2.zip)\n",
    "# - Upload Date: 05.04.2023\n",
    "# - Rows: 24,000\n",
    "# - Classes: 0 – non-harmful, 1 – harmful\n",
    "# - Moderation reasons: 4 pseudonymized classes representing moderation reasons\n",
    "df2: pandas.DataFrame = pandas.read_csv(filepaths.datasets / \"BAN-PL_2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the data\n",
    "\n",
    "Columns, rows, data types, missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_min_max(\n",
    "    df: pandas.DataFrame,\n",
    ") -> dict[str, tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Get min and max values for each numerical column in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame to get min and max values from.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, tuple[float, float]]: Dictionary with column names as keys and tuples with min and max values as values.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        column: (df[column].min(), df[column].max())\n",
    "        for column in df.select_dtypes(include=[numpy.number]).columns\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Number of rows:\n",
      "  df1 = 24,000\n",
      "  df2 = 24,000\n",
      "\n",
      "\n",
      "(2) Number of columns:\n",
      "  df1 = 2\n",
      "  df2 = 3\n",
      "\n",
      "\n",
      "(3) Column names:\n",
      "  df1 = ['Text', 'Class']\n",
      "  df2 = ['Text', 'Class', 'Reason']\n",
      "\n",
      "\n",
      "(4) Column range (min, max):\n",
      "  df1 = {'Class': (0, 1)}\n",
      "  df2 = {'Class': (0, 1), 'Reason': (1, 4)}\n",
      "\n",
      "\n",
      "(5) Column types:\n",
      "  df1 = {'Text': dtype('O'), 'Class': dtype('int64')}\n",
      "  df2 = {'Text': dtype('O'), 'Class': dtype('int64'), 'Reason': dtype('int64')}\n",
      "\n",
      "\n",
      "(6) Number of unique values in each column:\n",
      "  df1 = {'Text': 23985, 'Class': 2}\n",
      "  df2 = {'Text': 23985, 'Class': 2, 'Reason': 4}\n",
      "\n",
      "\n",
      "(7) Number of NA values in each column:\n",
      "  df1 = {'Text': 0, 'Class': 0}\n",
      "  df2 = {'Text': 0, 'Class': 0, 'Reason': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"(1) Number of rows:\\n  df1 = {len(df1):,}\\n  df2 = {len(df2):,}\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n\\n(2) Number of columns:\\n  df1 = {len(df1.columns):,}\\n  df2 = {len(df2.columns):,}\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n\\n(3) Column names:\\n  df1 = {df1.columns.tolist()}\\n  df2 = {df2.columns.tolist()}\",\n",
    ")\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"\\n\\n(4) Column range (min, max):\\n  df1 = {get_column_min_max(df1)}\\n  df2 = {get_column_min_max(df2)}\",\n",
    ")\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"\\n\\n(5) Column types:\\n  df1 = {df1.dtypes.to_dict()}\\n  df2 = {df2.dtypes.to_dict()}\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n\\n(6) Number of unique values in each column:\\n  df1 = {df1.nunique().to_dict()}\\n  df2 = {df2.nunique().to_dict()}\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n\\n(7) Number of NA values in each column:\\n  df1 = {df1.isna().sum().to_dict()}\\n  df2 = {df2.isna().sum().to_dict()}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First rows, random rows, last rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) First row:\n",
      "  df1 = [{'Text': 'Polska wtedy oficjalnie powinna przyznać, że oddadzą (oczywiście zgodnie z prawem po wyrokach sądów polskich prawowitym spadkobiercom) jak tylko Niemcy oddadzą za zniszczenia jakich dokonali w II WŚ na terenie Polski (⌐ ͡■ ͜ʖ ͡■)', 'Class': 0}]\n",
      "  df2 = [{'Text': 'Polska wtedy oficjalnie powinna przyznać, że oddadzą (oczywiście zgodnie z prawem po wyrokach sądów polskich prawowitym spadkobiercom) jak tylko Niemcy oddadzą za zniszczenia jakich dokonali w II WŚ na terenie Polski (⌐ ͡■ ͜ʖ ͡■)', 'Class': 0, 'Reason': 1}]\n",
      "\n",
      "\n",
      "(2) Last row:\n",
      "  df1 = [{'Text': 'Przecież murzynów nie da się szanować ani tolerować. Zawsze to samo. Jeden asfalt zostanie odstrzelony - reszta małp rozpie**** miasto. Gdzie tu logika?', 'Class': 1}]\n",
      "  df2 = [{'Text': 'Przecież murzynów nie da się szanować ani tolerować. Zawsze to samo. Jeden asfalt zostanie odstrzelony - reszta małp rozpie**** miasto. Gdzie tu logika?', 'Class': 1, 'Reason': 3}]\n",
      "\n",
      "\n",
      "(3) Random row:\n",
      "  df1 = [{'Text': '{USERNAME}: Nie wróci, będą po prostu niemieckie, irlandzkie albo holenderskie kible czyścić. Niech nas Bóg uchowa przed powrotem tych białych murzynów do Polski. Banda agresywnych, najebanych i roszczeniowych niedojebów.', 'Class': 1}]\n",
      "  df2 = [{'Text': '{USERNAME}: bo latwiej bylo bronic sie tam, niz kilkanascie km dalej.   A stracony teren trzeba tez potem bedzie odbic, znowu ponoszac straty.', 'Class': 0, 'Reason': 1}]\n",
      "\n",
      "\n",
      "(4) Random 10 rows (text only):\n",
      "  df1 = ['{USERNAME}: też miałem cywilny. Teść nie przyszedł. Taki obraz zjebanych katoli.', 'JA! POLEN= ARBEITEN, KEIN GELD', '{USERNAME}: pracowała w szpitalu, a tam przymusowa szpryca co 8 tygodni, albo jest 20 Ukrainek na twoje miejsce', '>Ktoś jeszcze ma wątpliwości że pis to ruska agentura?  {USERNAME}: powielasz kremlowską propagandę, oh wait ヽ( ͠°෴ °)ﾉ', '{USERNAME}: Przecież on pół wierchuszki PiS trzyma za jaja za pomocą teczek, niby kto miałby doprowadzić do tego, że dziad beknie?', '{USERNAME}: może i tak, ale wykopki lajkują ( ͡°( ͡° ͜ʖ( ͡° ͜ʖ ͡°)ʖ ͡°) ͡°)', '{USERNAME}: o! Widzę, że jeden z uczestników tego spędu bydła zabrał głos na tym portalu ze śmiesznymi obrazkami.', '{USERNAME}: małpy razem silne Szkoda chłopa, mam nadzieję że będzie bezpieczny w więzieniu', 'Na dzien dzisiejszy jak to sie mowi jebac wykopa  {URL} #[surname]', 'Teraz bedą na nich inne \"bobasy\" jeździć, kto wie, może jeszcze biedniejsze.']\n",
      "  df2 = ['{USERNAME}: do ruchania twojej matki też więc jestem spokojny', '{USERNAME} lol każda książka [surname] jest w topie listy sprzedaży, o czym ty pieprzysz matole?', '{USERNAME}: no to trzeba zgłosić sprzedawcę i wystawić jemu komentarz, a nie dodać ogólną ocenę do produktu i czekać na cud', '540 tys. osób lubi to! 659 tys. obserwujących! ...a to tylko Ci, co posiadają FB. ( ͡° ͜ʖ ͡°)', '{USERNAME}: Wulgarni znaczy widzą pedała mówią pedał i nie zaprzątają sobie głów głupotami?', 'Nie żal mi nikogo z budowlanki. Deweloperzy sprzedający dziury w ziemi 15k za metr i fachowcy z koziej dupy którzy chcieli po 5tys za metr mieszkania za wykończenie mieszkania i producenci materiałów którzy walili marże po 100%, albo i więcej. Niech się walą.', '{USERNAME}: W aurisie butla mi starczała na 850 km  - 1.8 hybrid', 'Nie zagłębiałem się w szczegóły. Ważne, że na 15 sędziów, 5 jest dublerami, a 2 źle obsadzona. Daje to 8, których wyroki teoretycznie są prawnie ok.  {USERNAME}: jednak siedzą razem z dublerami i razem z nimi biorą udział w obradach, a więc sami przyzwalają na bezprawie¯\\\\_(ツ)_/¯', '{USERNAME}: Gościu, jakbyś mi przyjebał w samochód przez swoje spierdolenie umysłowe to dostałbyś wypłatę w twarz. Użył byś gazu? Przez te parę sekund mógłbyś stracić wzrok lub być uduszonym bo potraktowałbym się za śmiertelne zagrożenie. Lepiej tak nie kozacz cwaniaczku rowerowy.  Aha, jakbyś jeździł po wawie rowerkiem i bym zobaczył że tak pajacujesz przed moim rowerem do innego auta to roweru byś nie odebrał do przyjazdu policji.', '{USERNAME} chuj z oponiarzem XD mam nadzieję że [surname] go zezlomuje jak skup pojazdów moje seicento']\n",
      "\n",
      "\n",
      "(5) Top 25 most common words:\n",
      "  df1 = {'to': 16118, 'nie': 15856, 'i': 15678, '{USERNAME}:': 14877, 'w': 14509, 'na': 12562, 'z': 10154, 'się': 9866, 'że': 7802, 'do': 6545, 'jest': 6368, 'jak': 6269, 'a': 5278, 'co': 4236, 'za': 3793, 'o': 3655, 'bo': 3608, 'ale': 3437, 'tak': 3001, 'po': 2970, '{USERNAME}': 2586, 'ma': 2529, '-': 2434, 'od': 2432, 'ze': 2390}\n",
      "  df2 = {'to': 16118, 'nie': 15856, 'i': 15678, '{USERNAME}:': 14877, 'w': 14509, 'na': 12562, 'z': 10154, 'się': 9866, 'że': 7802, 'do': 6545, 'jest': 6368, 'jak': 6269, 'a': 5278, 'co': 4236, 'za': 3793, 'o': 3655, 'bo': 3608, 'ale': 3437, 'tak': 3001, 'po': 2970, '{USERNAME}': 2586, 'ma': 2529, '-': 2434, 'od': 2432, 'ze': 2390}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"(1) First row:\\n  df1 = {df1.head(1).to_dict(orient='records')}\\n  df2 = {df2.head(1).to_dict(orient='records')}\",\n",
    ")\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"\\n\\n(2) Last row:\\n  df1 = {df1.tail(1).to_dict(orient='records')}\\n  df2 = {df2.tail(1).to_dict(orient='records')}\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n\\n(3) Random row:\\n  df1 = {df1.sample(1).to_dict(orient='records')}\\n  df2 = {df2.sample(1).to_dict(orient='records')}\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n\\n(4) Random 10 rows (text only):\\n  df1 = {[row['Text'] for row in df1.sample(10).to_dict(orient='records')]}\\n  df2 = {[row['Text'] for row in df2.sample(10).to_dict(orient='records')]}\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n\\n(5) Top 25 most common words:\\n  df1 = {df1['Text'].str.split(expand=True).stack().value_counts().head(25).to_dict()}\\n  df2 = {df2['Text'].str.split(expand=True).stack().value_counts().head(25).to_dict()}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hate-speech-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
